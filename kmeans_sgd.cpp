#include "kmeans_sgd.h"
#include <ctime>

void kMeansSGD::solve ( void ) {
   int size; MPI_Comm_size ( MPI_COMM_WORLD, &size );
   int rank; MPI_Comm_rank ( MPI_COMM_WORLD, &rank );

   // Parallelization: to improve the algorithm, we draw entries in batches. Each
   // process draws a portion of the batch and performs the algorithm
   // Since we don't want two processes to draw the same entry (waste of time and/or
   // possible conflicts) we have the random indexes generated by rank 0 in a
   // unique way, and then sent to the others
   const unsigned int batchSize = 20;

   std::default_random_engine eng(std::time(NULL));
   std::uniform_int_distribution<unsigned int> distro ( 0, dataset.size() - 1 );

   unsigned int iter = 0;

   // Count of elements in each class
   std::vector<unsigned int> counts ( k, 0 );

   // Initialize centroids with random elements in the dataset
   for ( unsigned int kk = 0; kk < k; ++kk )
      centroids[kk] = dataset[distro(eng)];

   while ( iter < 2 * dataset.size() / batchSize ) {
      // Pick random entries in the dataset
      std::vector<int> indices ( batchSize, -1 );
      if ( rank == 0 ) {
         indices[0] = distro(eng);
         for ( unsigned int i = 1; i < batchSize; ++i ) {
            do indices[i] = distro(eng);
            while ( find(indices.begin(), indices.begin() + i, indices[i]) != indices.begin() + i );
         }
      }

      MPI_Bcast ( indices.data(), batchSize, MPI_INT, 0, MPI_COMM_WORLD );

      for ( unsigned int i = rank; i < batchSize; i += size ) {
         unsigned int idx = indices[i];

         // Find the nearest centroid
         int nearestLabel = 0;
         real nearestDist = dist2 ( dataset[idx], centroids[0] );

         for ( unsigned int kk = 1; kk < k; ++kk ) {
            real d = dist2 ( dataset[idx], centroids[kk] );
            if ( d < nearestDist ) {
               nearestDist = d;
               nearestLabel = kk;
            }
         }

         if ( nearestLabel != dataset[idx].getLabel() ) {
            // Set the label of the picked point
            dataset[idx].setLabel(nearestLabel);
            // Update counts and centroids
            counts[nearestLabel] += 1;

            point diff = (dataset[idx] - centroids[nearestLabel]) / counts[nearestLabel];
            centroids[nearestLabel] += diff;
         }
      }

      // Processes communicate the changes
      for ( unsigned int i = 0; i < batchSize; ++i ) {
         if ( i % size == rank ) {
            int lab = dataset[indices[i]].getLabel();
            for ( int j = 0; j < size; ++j )
               if ( j != rank ) MPI_Send ( &lab, 1, MPI_INT, j, 0, MPI_COMM_WORLD );
         }

         else {
            int lab = 0;
            MPI_Recv ( &lab, 1, MPI_INT, i / size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE );
            dataset[indices[i]].setLabel ( lab );
         }
      }

      // Rank 0 updates centroids, then sends them to the others
      if ( rank == 0 ) {
         for ( unsigned int i = 0; i < batchSize; ++i ) {
            int label = dataset[indices[i]].getLabel();
            counts[label] += 1;

            point diff = ( dataset[indices[i]] - centroids[label] ) / counts[label];
            centroids[label] += diff;
         }

         // Send to everybody new centroids
         for ( int kk = 0; kk < k; ++kk )
            for ( int j = 1; j < size; ++j )
               mpi_point_send ( j, centroids[kk] );
      }

      // Other ranks: receive centroids from rank 0
      else {
         for ( int kk = 0; kk < k; ++kk )
            centroids[kk] = mpi_point_recv ( 0, n );
      }

      // Communicate counts
      MPI_Bcast ( counts.data(), k, MPI_UNSIGNED, 0, MPI_COMM_WORLD );

      // Stopping criterion ?

      ++iter;
   }
}
